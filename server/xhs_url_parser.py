"""
å°çº¢ä¹¦é“¾æ¥è§£æå·¥å…·
æ”¯æŒå¤šç§æ ¼å¼çš„é“¾æ¥è§£æï¼Œç»Ÿä¸€è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
"""
import re
import requests
from typing import Optional, Tuple
from urllib.parse import urlparse, parse_qs
import logging

logger = logging.getLogger(__name__)


def extract_note_info_from_url(url: str, cookie: Optional[str] = None) -> Tuple[Optional[str], Optional[str], Optional[str]]:
    """
    ä»å°çº¢ä¹¦é“¾æ¥ä¸­æå–ç¬”è®°IDå’Œxsec_token

    æ”¯æŒçš„æ ¼å¼ï¼š
    1. çŸ­é“¾æ¥: http://xhslink.com/o/xxxxx
    2. discoveryé“¾æ¥: https://www.xiaohongshu.com/discovery/item/{note_id}?xsec_token=xxx
    3. exploreé“¾æ¥: https://www.xiaohongshu.com/explore/{note_id}?xsec_token=xxx
    4. ç™»å½•é‡å®šå‘: https://www.xiaohongshu.com/login?redirectPath=https://www.xiaohongshu.com/discovery/item/{note_id}?...

    Args:
        url: åŸå§‹URLï¼ˆå¯èƒ½åŒ…å«åœ¨æ–‡æœ¬ä¸­ï¼‰
        cookie: å°çº¢ä¹¦cookieï¼ˆç”¨äºè§£æçŸ­é“¾æ¥ï¼‰

    Returns:
        (note_id, xsec_token, normalized_url)
    """
    if not url:
        return None, None, None

    # æå–URLï¼ˆä»æ–‡æœ¬ä¸­ï¼‰
    url = extract_url_from_text(url)
    if not url:
        logger.warning("æ— æ³•ä»è¾“å…¥æ–‡æœ¬ä¸­æå–URL")
        return None, None, None

    # æ£€æŸ¥æ˜¯å¦ä¸ºçŸ­é“¾æ¥
    if is_short_link(url):
        logger.info(f"æ£€æµ‹åˆ°çŸ­é“¾æ¥: {url}")
        url = resolve_short_link(url, cookie)
        if not url:
            logger.error("çŸ­é“¾æ¥è§£æå¤±è´¥")
            return None, None, None
        logger.info(f"çŸ­é“¾æ¥è§£æç»“æœ: {url}")

    # æ£€æŸ¥æ˜¯å¦ä¸ºç™»å½•é‡å®šå‘URLï¼ˆçŸ­é“¾æ¥å¯èƒ½é‡å®šå‘åˆ°ç™»å½•é¡µï¼‰
    if '/login' in url and 'redirectPath=' in url:
        logger.info(f"æ£€æµ‹åˆ°ç™»å½•é‡å®šå‘URL")
        redirect_url = extract_redirect_path(url)
        if redirect_url:
            logger.info(f"ä» redirectPath æå–åˆ°ç›®æ ‡URL")
            url = redirect_url
        else:
            logger.warning("æ— æ³•ä»ç™»å½•é‡å®šå‘URLä¸­æå– redirectPath")

    # æå–ç¬”è®°ID
    note_id = extract_note_id(url)
    if not note_id:
        logger.warning(f"æ— æ³•ä»URLä¸­æå–ç¬”è®°ID: {url}")
        return None, None, None

    # æå–xsec_token
    xsec_token = extract_xsec_token(url)
    if not xsec_token:
        logger.warning(f"æ— æ³•ä»URLä¸­æå–xsec_token: {url}")
        return None, None, None

    # ç”Ÿæˆæ ‡å‡†åŒ–URL
    normalized_url = f"https://www.xiaohongshu.com/explore/{note_id}?xsec_token={xsec_token}"

    return note_id, xsec_token, normalized_url


def extract_url_from_text(text: str) -> Optional[str]:
    """
    ä»æ–‡æœ¬ä¸­æå–URL

    ç¤ºä¾‹è¾“å…¥:
    "å¯¹æ–¹ä¿é™©å…¬å¸è¦æˆ‘ä»¬èµ”5w å¯¹æ–¹è½¦æŸä¸€ä¸ªç¯ç½©ï¼Œæ‰æ¼†ï¼Œè¿˜... http://xhslink.com/o/9GubyGk1LPj"
    "63%20ã€æ ‡é¢˜ã€‘ ğŸ˜† https://www.xiaohongshu.com/discovery/item/xxx?xsec_token=xxx"

    Returns:
        æå–åˆ°çš„URL
    """
    # å…ˆè§£ç  URL ç¼–ç ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    import urllib.parse
    try:
        decoded_text = urllib.parse.unquote(text)
    except Exception:
        decoded_text = text

    # åŒ¹é… xiaohongshu.com åŸŸåçš„å®Œæ•´URLï¼ˆä¼˜å…ˆï¼‰
    # è¿™ä¸ªæ¨¡å¼ä¼šåŒ¹é…ä» http/https å¼€å§‹ï¼Œåˆ°ä¸‹ä¸€ä¸ªç©ºæ ¼ã€ä¸­æ–‡æˆ–å­—ç¬¦ä¸²ç»“å°¾
    xhs_pattern = r'https?://(?:www\.)?xiaohongshu\.com/[^\s\u4e00-\u9fff]*'
    xhs_matches = re.findall(xhs_pattern, decoded_text)

    if xhs_matches:
        # æ¸…ç† URL æœ«å°¾å¯èƒ½çš„æ ‡ç‚¹ç¬¦å·
        url = xhs_matches[0].rstrip('.,;!?ã€‚ï¼Œï¼›ï¼ï¼Ÿ')
        return url

    # åŒ¹é…çŸ­é“¾æ¥æˆ–å…¶ä»– http/https é“¾æ¥
    url_pattern = r'https?://[^\s\u4e00-\u9fff]+'
    matches = re.findall(url_pattern, decoded_text)

    if matches:
        # æ¸…ç† URL æœ«å°¾å¯èƒ½çš„æ ‡ç‚¹ç¬¦å·
        url = matches[0].rstrip('.,;!?ã€‚ï¼Œï¼›ï¼ï¼Ÿ')
        return url

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°URLï¼Œè¿”å›åŸå§‹æ–‡æœ¬ï¼ˆå¯èƒ½æœ¬èº«å°±æ˜¯URLï¼‰
    return text.strip()


def extract_redirect_path(url: str) -> Optional[str]:
    """
    ä»ç™»å½•é‡å®šå‘URLä¸­æå–redirectPathå‚æ•°

    ç¤ºä¾‹è¾“å…¥:
    https://www.xiaohongshu.com/login?redirectPath=https://www.xiaohongshu.com/discovery/item/xxx?xsec_token=yyy

    Returns:
        æå–åˆ°çš„redirectPath URL
    """
    try:
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå– redirectPathï¼Œå› ä¸ºå®ƒçš„å€¼æœ¬èº«å°±æ˜¯ä¸€ä¸ªå®Œæ•´çš„ URL
        # ç›´æ¥ç”¨ parse_qs ä¼šæŠŠå†…åµŒ URL çš„å‚æ•°ä¹Ÿæ‹†åˆ†å‡ºæ¥
        redirect_match = re.search(r'redirectPath=([^&\s]+(?:&[^&\s]+)*)', url)
        if redirect_match:
            # è·å– redirectPath åé¢çš„æ‰€æœ‰å†…å®¹ï¼Œç›´åˆ°é‡åˆ°ä¸å±äº URL çš„å­—ç¬¦
            redirect_start = redirect_match.start(1)
            # ä» redirectPath çš„å€¼å¼€å§‹ï¼Œä¸€ç›´åˆ°å­—ç¬¦ä¸²ç»“å°¾æˆ–ç©ºæ ¼
            remaining = url[redirect_start:]

            # redirectPath çš„å€¼åº”è¯¥æ˜¯ä» https:// å¼€å§‹çš„å®Œæ•´ URL
            # æˆ‘ä»¬éœ€è¦æ‰¾åˆ°å®ƒçš„ç»“æŸä½ç½®
            # ç»“æŸæ ‡å¿—ï¼šç©ºæ ¼ã€å¼•å·ã€æˆ–è€…å­—ç¬¦ä¸²ç»“å°¾
            redirect_url = remaining.split()[0].rstrip('",;')

            # URL è§£ç ï¼ˆredirectPath çš„å€¼å¯èƒ½æ˜¯ç¼–ç çš„ï¼Œå¦‚ %3A %2Fï¼‰
            import urllib.parse
            try:
                decoded_url = urllib.parse.unquote(redirect_url)
                logger.info(f"ä»redirectPathæå–URLï¼ˆå·²è§£ç ï¼‰: {decoded_url[:100]}...")
                return decoded_url
            except Exception:
                logger.info(f"ä»redirectPathæå–URL: {redirect_url[:100]}...")
                return redirect_url

        # å¦‚æœæ­£åˆ™åŒ¹é…å¤±è´¥ï¼Œå°è¯•ä¼ ç»Ÿæ–¹æ³•
        parsed = urlparse(url)
        query_params = parse_qs(parsed.query)

        if 'redirectPath' in query_params:
            # parse_qs ä¼šæ‹†åˆ†å†…åµŒçš„å‚æ•°ï¼Œæˆ‘ä»¬éœ€è¦é‡æ–°ç»„è£…
            # æ³¨æ„ï¼šparse_qs ä¼šè‡ªåŠ¨è§£ç ï¼Œæ‰€ä»¥ base_redirect å·²ç»æ˜¯è§£ç åçš„
            base_redirect = query_params['redirectPath'][0]

            # æ”¶é›†æ‰€æœ‰å¯èƒ½å±äº redirectPath çš„å‚æ•°
            redirect_params = {}
            for key in ['app_platform', 'app_version', 'share_from_user_hidden',
                       'xsec_source', 'type', 'xsec_token', 'author_share',
                       'xhsshare', 'shareRedId', 'apptime', 'share_id', 'exSource']:
                if key in query_params:
                    redirect_params[key] = query_params[key][0]

            # é‡æ–°ç»„è£… redirectPath
            if redirect_params:
                params_list = [f"{k}={v}" for k, v in redirect_params.items()]
                param_str = '&'.join(params_list)
                redirect_url = base_redirect + '&' + param_str
                logger.info(f"é‡æ–°ç»„è£…redirectPathï¼ˆå·²è§£ç ï¼‰: {redirect_url[:100]}...")
                return redirect_url
            else:
                logger.info(f"æå–redirectPathï¼ˆå·²è§£ç ï¼‰: {base_redirect[:100]}...")
                return base_redirect

        return None
    except Exception as e:
        logger.error(f"æå–redirectPathå¤±è´¥: {e}")
        return None


def is_short_link(url: str) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦ä¸ºå°çº¢ä¹¦çŸ­é“¾æ¥
    """
    return 'xhslink.com' in url.lower()


def resolve_short_link(short_url: str, cookie: Optional[str] = None, timeout: int = 10) -> Optional[str]:
    """
    è§£æå°çº¢ä¹¦çŸ­é“¾æ¥ï¼Œè·å–çœŸå®çš„é•¿é“¾æ¥

    Args:
        short_url: çŸ­é“¾æ¥ (å¦‚: http://xhslink.com/o/9GubyGk1LPj)
        cookie: å°çº¢ä¹¦cookie
        timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

    Returns:
        çœŸå®çš„é•¿é“¾æ¥
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://www.xiaohongshu.com/',
        }

        if cookie:
            headers['Cookie'] = cookie

        # å‘é€è¯·æ±‚ï¼Œä¸è‡ªåŠ¨è·Ÿéšé‡å®šå‘
        response = requests.get(
            short_url,
            headers=headers,
            allow_redirects=True,  # è‡ªåŠ¨è·Ÿéšé‡å®šå‘
            timeout=timeout
        )

        # å¦‚æœå‘ç”Ÿäº†é‡å®šå‘ï¼Œè¿”å›æœ€ç»ˆURL
        if response.history:
            final_url = response.url
            logger.info(f"çŸ­é“¾æ¥é‡å®šå‘: {short_url} -> {final_url}")
            return final_url

        # å¦‚æœæ²¡æœ‰é‡å®šå‘ï¼Œæ£€æŸ¥å“åº”ä½“ä¸­æ˜¯å¦åŒ…å«é“¾æ¥
        if response.status_code == 200:
            # å°è¯•ä»å“åº”ä¸­æå–é“¾æ¥
            text = response.text
            url_match = re.search(r'https://www\.xiaohongshu\.com/[^\s"\'<>]+', text)
            if url_match:
                return url_match.group(0)

        logger.warning(f"çŸ­é“¾æ¥è§£æå¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
        return None

    except requests.exceptions.Timeout:
        logger.error(f"çŸ­é“¾æ¥è¯·æ±‚è¶…æ—¶: {short_url}")
        return None
    except Exception as e:
        logger.error(f"çŸ­é“¾æ¥è§£æå¼‚å¸¸: {e}")
        return None


def extract_note_id(url: str) -> Optional[str]:
    """
    ä»URLä¸­æå–ç¬”è®°ID

    æ”¯æŒçš„æ ¼å¼ï¼š
    - https://www.xiaohongshu.com/explore/{note_id}
    - https://www.xiaohongshu.com/discovery/item/{note_id}
    """
    # å°è¯•ä»exploreè·¯å¾„æå–
    explore_pattern = r'/explore/([a-f0-9]{24})'
    match = re.search(explore_pattern, url)
    if match:
        return match.group(1)

    # å°è¯•ä»discovery/itemè·¯å¾„æå–
    discovery_pattern = r'/discovery/item/([a-f0-9]{24})'
    match = re.search(discovery_pattern, url)
    if match:
        return match.group(1)

    return None


def extract_xsec_token(url: str) -> Optional[str]:
    """
    ä»URLæŸ¥è¯¢å‚æ•°ä¸­æå–xsec_token
    """
    try:
        parsed = urlparse(url)
        query_params = parse_qs(parsed.query)

        # è·å–xsec_tokenå‚æ•°
        if 'xsec_token' in query_params:
            tokens = query_params['xsec_token']
            if tokens and len(tokens) > 0:
                return tokens[0]

        return None
    except Exception as e:
        logger.error(f"æå–xsec_tokenå¤±è´¥: {e}")
        return None


def normalize_xhs_url(input_text: str, cookie: Optional[str] = None) -> Optional[str]:
    """
    ä¾¿æ·æ–¹æ³•ï¼šæ ‡å‡†åŒ–å°çº¢ä¹¦é“¾æ¥

    Args:
        input_text: è¾“å…¥æ–‡æœ¬ï¼ˆå¯èƒ½åŒ…å«é“¾æ¥ï¼‰
        cookie: å°çº¢ä¹¦cookie

    Returns:
        æ ‡å‡†åŒ–çš„URL: https://www.xiaohongshu.com/explore/{note_id}?xsec_token={token}
    """
    note_id, xsec_token, normalized_url = extract_note_info_from_url(input_text, cookie)
    return normalized_url


if __name__ == "__main__":
    # æµ‹è¯•ç”¨ä¾‹
    import sys
    import io

    # ä¿®å¤Windowsæ§åˆ¶å°ç¼–ç é—®é¢˜
    if sys.platform == 'win32':
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")

    # æµ‹è¯•ç”¨ä¾‹1: çŸ­é“¾æ¥
    test_url_1 = "å¯¹æ–¹ä¿é™©å…¬å¸è¦æˆ‘ä»¬èµ”5w å¯¹æ–¹è½¦æŸä¸€ä¸ªç¯ç½©ï¼Œæ‰æ¼†ï¼Œè¿˜... http://xhslink.com/o/9GubyGk1LPj å¤åˆ¶åæ‰“å¼€ã€å°çº¢ä¹¦ã€‘æŸ¥çœ‹ç¬”è®°ï¼"
    logger.info(f"æµ‹è¯•1 è¾“å…¥: {test_url_1}")
    # éœ€è¦cookieæ‰èƒ½è§£æçŸ­é“¾æ¥
    # result_1 = normalize_xhs_url(test_url_1, cookie='your_cookie_here')
    # logger.info(f"æµ‹è¯•1 ç»“æœ: {result_1}\n")

    # æµ‹è¯•ç”¨ä¾‹2: discoveryé“¾æ¥
    test_url_2 = "https://www.xiaohongshu.com/discovery/item/691640ea0000000007022395?source=webshare&xhsshare=pc_web&xsec_token=CBr0pTTp8vm5CarMUCnZfuPTHwVMNXGXjnvPvI9NvsgqQ=&xsec_source=pc_share"
    logger.info(f"æµ‹è¯•2 è¾“å…¥: {test_url_2}")
    result_2 = normalize_xhs_url(test_url_2)
    logger.info(f"æµ‹è¯•2 ç»“æœ: {result_2}\n")

    # æµ‹è¯•ç”¨ä¾‹3: exploreé“¾æ¥
    test_url_3 = "https://www.xiaohongshu.com/explore/691640ea0000000007022395?app_platform=ios&app_version=9.6&share_from_user_hidden=true&xsec_source=app_share&type=normal&xsec_token=CBr0pTTp8vm5CarMUCnZfuPTHwVMNXGXjnvPvI9NvsgqQ=&author_share=1&xhsshare=CopyLink&shareRedId=ODk5RjU3RU42NzUyOTgwNjdJOTg6Rz1B&apptime=1763089039&share_id=0840e2340e9d421597831a18b2c2acbb"
    logger.info(f"æµ‹è¯•3 è¾“å…¥: {test_url_3}")
    result_3 = normalize_xhs_url(test_url_3)
    logger.info(f"æµ‹è¯•3 ç»“æœ: {result_3}\n")
